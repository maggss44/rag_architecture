{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosmayorga/anaconda3/envs/RAG_ARCHITECTURE/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/carlosmayorga/anaconda3/envs/RAG_ARCHITECTURE/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <ABE0EE74-6D97-3B8C-B690-C44754774FBC> /Users/carlosmayorga/anaconda3/envs/RAG_ARCHITECTURE/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <EEB3232B-F6A7-3262-948C-BB2F54905803> /Users/carlosmayorga/anaconda3/envs/RAG_ARCHITECTURE/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Carga el archivo .env\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Document_Processing:\n",
    "    def __init__(self, file_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize the Document_Processing class with a list of file paths.\n",
    "\n",
    "        Args:\n",
    "            file_paths (List[str]): A list of file paths to PDF files.\n",
    "        \"\"\"\n",
    "        self.documents = file_paths\n",
    "\n",
    "    def preprocess_documents(self, file_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocess a single document by loading and splitting it into smaller chunks.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the PDF file to be processed.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of text chunks from the PDF document.\n",
    "        \"\"\"\n",
    "        # Create a loader for the PDF document\n",
    "        loader = PyPDFLoader(file_path)\n",
    "\n",
    "        # Load the data from the PDF\n",
    "        data = loader.load()\n",
    "\n",
    "        # Split the document into chunks (e.g., 1000 characters)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        documents = text_splitter.split_documents(data)\n",
    "\n",
    "        # Convert Document objects into strings\n",
    "        text_chunks = [str(doc) for doc in documents]\n",
    "\n",
    "        return text_chunks\n",
    "\n",
    "    def process_documents(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process all documents in the list of file paths and return their preprocessed text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of preprocessed text chunks from all documents.\n",
    "        \"\"\"\n",
    "        all_processed_documents = []\n",
    "\n",
    "        # Preprocess each document in the file paths\n",
    "        for file_path in self.documents:\n",
    "            processed_text = self.preprocess_documents(file_path)\n",
    "            all_processed_documents.extend(processed_text)\n",
    "\n",
    "        return all_processed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    def __init__(self, data: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the DataSource object with a list of data.\n",
    "\n",
    "        Args:\n",
    "            data (List[str]): The input list of text data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, and removing extra whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "        return text\n",
    "\n",
    "    def preprocess_text_advanced(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, removing extra whitespace,\n",
    "        removing numbers, removing stop words, and lemmatizing the words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed and cleaned text.\n",
    "        \"\"\"\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower() \n",
    "        # Remove punctuation and numeric characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Lemmatize the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # Join the tokens back into a string\n",
    "        text = ' '.join(tokens)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the preprocessed text into a list of words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The preprocessed text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of tokens (words) from the text.\n",
    "        \"\"\"\n",
    "        return self.preprocess_text_advanced(text).split()  # Tokenize the preprocessed text by splitting on spaces\n",
    "\n",
    "    def process_data(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes the data by applying advanced preprocessing to each sentence in the data list.\n",
    "\n",
    "        Updates the object's processed_data attribute with the cleaned and preprocessed text.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.processed_data = [self.preprocess_text_advanced(sentence) for sentence in self.data] # Apply preprocessing to each sentence in the data list\n",
    "\n",
    "        return self.processed_data  # Apply preprocessing to each sentence in the data list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserQuery:\n",
    "    def __init__(self, query: str):\n",
    "        self.query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, model_name: str, device: str = 'cpu', use_local: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the Embedding object with the specified model name, device, and whether to use a local model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the embedding model to be used.\n",
    "            device (str): The device to be used for running the model (\"cpu\" or \"cuda\"). Default is \"cpu\".\n",
    "            use_local (bool): Whether to use a locally available model. Default is False.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.use_local = use_local\n",
    "        # Mapping of model names to their expected dimensions\n",
    "        self.model_dimensions = {\n",
    "            'all-MiniLM-L6-v2': 384,  # Example dimension for a SentenceTransformer model\n",
    "            'jina-v2-base-en-embed': 768,  # Specified dimension for your local model\n",
    "            # Add other models and their dimensions here\n",
    "        }\n",
    "        self.current_model_dimension = self.model_dimensions.get(model_name, None)\n",
    "\n",
    "        if use_local:\n",
    "            raise ValueError(f\"Not yet implemented\")\n",
    "        else:\n",
    "            self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches the embedding model to a different model and device.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new embedding model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        # Here we assume the model switch is successful and just set the dimension\n",
    "        if model_name in self.model_dimensions:\n",
    "            self.current_model_dimension = self.model_dimensions[model_name]\n",
    "            print(f\"Switched to model '{model_name}' with dimension {self.current_model_dimension}\")\n",
    "        else:\n",
    "            print(f\"Model '{model_name}' not recognized. Unable to switch models.\")\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embeds the input text into a vector representation using the current embedding model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The vector representation of the input text.\n",
    "        \"\"\"\n",
    "        if self.use_local:\n",
    "          raise ValueError(f\"Not yet implemented!\")\n",
    "        else:\n",
    "            result = self.model.encode(text).tolist()\n",
    "            if len(result) != self.current_model_dimension:\n",
    "                print(f\"Dimension mismatch detected: Expected {self.current_model_dimension}, got {len(result)}\")\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStorage:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]]):\n",
    "        # Placeholder method for storing vectors\n",
    "        pass\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int) -> List[int]:\n",
    "        # Placeholder method for searching vectors\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeVectorStorage(VectorStorage):\n",
    "    def __init__(self, index_name: str, embedding: Embedding):\n",
    "        \"\"\"\n",
    "        Initializes the PineconeVectorStorage object with the specified index name and embedding model.\n",
    "\n",
    "        Args:\n",
    "            index_name (str): The name of the Pinecone index to be created or used.\n",
    "            embedding (Embedding): An instance of the Embedding class providing the embedding model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "        self.pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        if index_name not in self.pinecone.list_indexes().names():\n",
    "            self.pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=embedding.model.get_sentence_embedding_dimension(),\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "            )\n",
    "        self.index = self.pinecone.Index(index_name)\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]], metadatas: List[dict]):\n",
    "        \"\"\"\n",
    "        Stores vectors and associated metadata in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            vectors (List[List[float]]): List of vectors to be stored in the index.\n",
    "            metadatas (List[dict]): List of dictionaries containing metadata associated with each vector.\n",
    "        \"\"\"\n",
    "        ids = [str(i) for i in range(len(vectors))]\n",
    "        records = zip(ids, vectors, metadatas)\n",
    "        self.index.upsert(vectors=records)\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int, filter_metadata: Dict[str, str] = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Searches for vectors similar to the given query vector in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            query_vector (List[float]): The query vector for similarity search.\n",
    "            top_n (int): The number of top results to return.\n",
    "            filter_metadata (Dict[str, str], optional): Metadata filters to apply during search. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: List of dictionaries containing metadata of the top similar vectors.\n",
    "        \"\"\"\n",
    "        query_params = {\n",
    "            'top_k': top_n,\n",
    "            'vector': query_vector,\n",
    "            'include_metadata': True,\n",
    "            'include_values': False\n",
    "        }\n",
    "        if filter_metadata is not None:\n",
    "            query_params['metadata'] = filter_metadata\n",
    "\n",
    "        results = self.index.query(**query_params)\n",
    "        return results['matches']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAndRanking:\n",
    "    def __init__(self, embedding: Embedding, vector_storage: VectorStorage):\n",
    "        \"\"\"\n",
    "        Initializes the RetrievalAndRanking object with a data source, embedding model, and vector storage.\n",
    "\n",
    "        Args:\n",
    "            data_source (DataSource): The data source containing the preprocessed text data.\n",
    "            embedding (Embedding): The embedding model used to generate vector representations of text.\n",
    "            vector_storage (VectorStorage): The storage for vector representations of text data.\n",
    "        \"\"\"\n",
    "        self.embedding = embedding\n",
    "        self.vector_storage = vector_storage\n",
    "\n",
    "    def retrieve_relevant_chunks_pinecone(self, queries: List[str], top_n: int = 2, filter_metadata: Dict[str, str] = None) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Pinecone vector storage.\n",
    "\n",
    "        Args:\n",
    "            queries (List[str]): A list of user queries.\n",
    "            top_n (int): The number of top relevant chunks to retrieve for each query (default is 2).\n",
    "            filter_metadata (Dict[str, str], optional): A dictionary containing metadata key-value pairs to filter the results (default is None).\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A list containing the top-n most relevant chunks for each query.\n",
    "        \"\"\"\n",
    "        relevant_chunks = []\n",
    "\n",
    "        for query in queries:\n",
    "            query_embedding = self.embedding.embed(query)\n",
    "            results = self.vector_storage.search_vectors(query_embedding, top_n)\n",
    "\n",
    "            filtered_results = []\n",
    "            if filter_metadata is not None:\n",
    "                for result in results:\n",
    "                    metadata = result.get('metadata', {})\n",
    "                    if all(metadata.get(key) == value for key, value in filter_metadata.items()):\n",
    "                        filtered_results.append(result)\n",
    "            else:\n",
    "                filtered_results = results\n",
    "\n",
    "            relevant_chunks.append([result['metadata']['text'] for result in filtered_results])\n",
    "\n",
    "        return relevant_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, api_key: str = None, model_name: str = None, device: str = \"cuda\", consumer_group: str = \"mistral\"):\n",
    "        \"\"\"\n",
    "        Initializes the LLM object with the specified API key, model name, device, and consumer group.\n",
    "        Sets up the OpenAI client if an API key is provided.\n",
    "\n",
    "        Args:\n",
    "            api_key (str, optional): The API key for accessing the OpenAI service. Default is None.\n",
    "            model_name (str, optional): The name of the model to be used. Default is None.\n",
    "            device (str, optional): The device to be used for running the model (\"cuda\" or \"cpu\"). Default is \"cuda\".\n",
    "            consumer_group (str, optional): The consumer group to be used. Default is \"mistral\".\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.consumer_group = consumer_group\n",
    "\n",
    "        if self.api_key:\n",
    "            self.client = OpenAI(api_key=self.api_key)  # Set up the OpenAI client with the provided API key\n",
    "        else:\n",
    "            print('Coming Soon')  # Placeholder message for when API key is not provided\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches to a different model and device for the LLM. Creates a new reader if necessary.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        current_readers = self.takeoff_client.get_readers()  # Retrieve the current readers from the takeoff client\n",
    "\n",
    "        # Check if a reader for the desired model already exists\n",
    "        reader_id = None\n",
    "        for group, readers in current_readers.items():\n",
    "            for reader in readers:\n",
    "                if reader['model_name'] == model_name:  # Check if the desired model is already in use\n",
    "                    reader_id = reader['reader_id']\n",
    "                    break\n",
    "            if reader_id:\n",
    "                break\n",
    "\n",
    "        if reader_id:\n",
    "            print(f\"Reader for model '{model_name}' already exists with reader_id: {reader_id}\")\n",
    "        else:\n",
    "            reader_config = {\n",
    "                \"model_name\": model_name,\n",
    "                \"device\": device,\n",
    "                \"consumer_group\": self.consumer_group\n",
    "            }\n",
    "\n",
    "            reader_id, _ = self.takeoff_client.create_reader(reader_config=reader_config)  # Create a new reader\n",
    "            print(f\"Created a new reader with reader_id {reader_id}\")\n",
    "\n",
    "    def answer_query(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates an answer to a query based on the provided context using the LLM.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            context (str): The context to be used for answering the query.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer to the query.\n",
    "        \"\"\"\n",
    "        prompt = f\"Based on the provided context, answer the following query: {query}\\n\\nContext:\\n{context}. Do not use your knowledge, only the context\"\n",
    "        \n",
    "        if self.api_key:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo\",  # Specify the model to be used for generating the response\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()  # Return the generated response\n",
    "        else:\n",
    "            response = self.takeoff_client.generate(prompt, consumer_group=self.consumer_group)  # Generate response using the takeoff client\n",
    "            if 'text' in response:\n",
    "                return response['text'].strip()  # Return the generated response\n",
    "            else:\n",
    "                print(f\"Error generating response: {response}\")\n",
    "                return \"Unable to generate a response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(queries: List[str], retrieval_and_ranking: RetrievalAndRanking, llm: LLM, retrieval_method: str = \"default\") -> List[str]:\n",
    "    user_queries = [UserQuery(query) for query in queries]\n",
    "    answers = []\n",
    "\n",
    "    for user_query in user_queries:\n",
    "        if retrieval_method == \"default\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks(user_query.query)\n",
    "            context = \"\\n\".join(relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        elif retrieval_method == \"euclidean\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_euclidean(user_query.query)\n",
    "            context = \"\\n\".join(relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        elif retrieval_method == \"pinecone\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_pinecone([user_query.query])\n",
    "            context = \"\\n\".join(\"\\n\".join(chunks) for chunks in relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retrieval method: {retrieval_method}\")\n",
    "\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(retrieval_method: str = \"default\",\n",
    "         model_choice: str = \"openai\",\n",
    "         model_name: str = None,\n",
    "         device: str = \"cpu\",\n",
    "         embedding_model_name = 'all-MiniLM-L6-v2',\n",
    "         use_local=False,\n",
    "         index_name=\"my-index\"\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Main function for running the retrieval and ranking system with user interaction.\n",
    "\n",
    "    Args:\n",
    "        data_source (DataSource): An instance of the DataSource class providing the data.\n",
    "        retrieval_method (str, optional): The method to use for retrieval and ranking. Defaults to \"default\".\n",
    "        model_choice (str, optional): The choice of model for language understanding. Defaults to \"openai\".\n",
    "        model_name (str, optional): The name of the model to be used. Defaults to None.\n",
    "        device (str, optional): The device to be used for embedding. Defaults to \"cpu\".\n",
    "        embedding_model_name (str, optional): The name of the embedding model to be used. Defaults to 'all-MiniLM-L6-v2'.\n",
    "        use_local (bool, optional): Whether to use a local model for embedding. Defaults to False.\n",
    "        index_name (str, optional): The name of the index for vector storage. Defaults to \"my-index\".\n",
    "    \"\"\"\n",
    "    embedding = Embedding(model_name=embedding_model_name,\n",
    "                          device=device,\n",
    "                          use_local=use_local)\n",
    "    vector_storage = PineconeVectorStorage(index_name, embedding)\n",
    "    \n",
    "\n",
    "    retrieval_and_ranking = RetrievalAndRanking(embedding, vector_storage)\n",
    "\n",
    "    if model_choice == \"openai\":\n",
    "        llm = LLM(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Not yet implemented\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        queries = user_input.split(\";\")  # Split multiple queries separated by semicolon\n",
    "        answers = process_query(queries, retrieval_and_ranking, llm, retrieval_method)\n",
    "\n",
    "        print(\"User Queries:\")\n",
    "        for query, answer in zip(queries, answers):\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_new_documents(data_source:DataSource,\n",
    "         device: str = \"cpu\",\n",
    "         embedding_model_name = 'all-MiniLM-L6-v2',\n",
    "         use_local=False,\n",
    "         index_name=\"my-index\"\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Main function for running the retrieval and ranking system with user interaction.\n",
    "\n",
    "    Args:\n",
    "        data_source (DataSource): An instance of the DataSource class providing the data.\n",
    "        retrieval_method (str, optional): The method to use for retrieval and ranking. Defaults to \"default\".\n",
    "        model_choice (str, optional): The choice of model for language understanding. Defaults to \"openai\".\n",
    "        model_name (str, optional): The name of the model to be used. Defaults to None.\n",
    "        device (str, optional): The device to be used for embedding. Defaults to \"cpu\".\n",
    "        embedding_model_name (str, optional): The name of the embedding model to be used. Defaults to 'all-MiniLM-L6-v2'.\n",
    "        use_local (bool, optional): Whether to use a local model for embedding. Defaults to False.\n",
    "        index_name (str, optional): The name of the index for vector storage. Defaults to \"my-index\".\n",
    "    \"\"\"\n",
    "    embedding = Embedding(model_name=embedding_model_name,\n",
    "                          device=device,\n",
    "                          use_local=use_local)\n",
    "    vector_storage = PineconeVectorStorage(index_name, embedding)\n",
    "    processed_data = data_source.process_data()\n",
    "    metadatas = [{'text': text, 'category': 'politics'} for text in processed_data]\n",
    "    vectors = [embedding.embed(text) for text in processed_data]\n",
    "    vector_storage.store_vectors(vectors, metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedings Vector Mapper\n",
    "Map new pdf documents to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_documants = Document_Processing([\"./data/Choi et al.pdf\"])\n",
    "# data = text_documants.process_documents()\n",
    "# data_source = DataSource(data)\n",
    "# data_source.process_data()\n",
    "# store_new_documents(data_source, index_name=\"my-index-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Queries:\n",
      "Query: how is power rooted?\n",
      "Answer: Power is rooted in both formal and informal sources within a network. Formal power is derived from one's position or authority within a hierarchy, such as pulling rank or making decisions based on one's formal position. On the other hand, informal power comes from holding a central position within a network, allowing for greater influence and access to valuable resources. Power can also be rooted in personal attributes, cognitive abilities, technical expertise, shared resources, and social relationships within the network. Overall, power in a network is influenced by the structural configuration and interactions among its members.\n",
      "\n",
      "User Queries:\n",
      "Query: what is political based power?\n",
      "Answer: Political-based power refers to a form of influence and authority that stems from individuals or organizations having astute knowledge of the political dynamics within a network. This type of power is acquired through actions such as cooptation and negotiation, based on an understanding of organizational politics. In the context provided, it is suggested that political-based power within an emergency network is assessed by the degree of political knowledge present in the collective, allowing individuals or organizations to leverage their understanding of network links and central actors for strategic advantage.\n",
      "\n",
      "User Queries:\n",
      "Query: in the paper \"Power and Cognitive Accuracy in Local Emergency Management Networks\", what is said about Cognitive Accuracy in Networks? is the \n",
      "Answer: The paper discusses cognitive accuracy in local emergency management networks and how participant collective cognitive accuracy within a network can impact its effectiveness. It mentions that collective cognitive accuracy is operationalized based on Carley's notion of a common operational picture. If individuals within a network have faulty perceptions, it can lead to communication difficulties. On the other hand, if participants perceive the network structure accurately, they are likely to share an accurate operational picture. The clarity regarding central actors and needed information within the network is crucial for effective performance. This understanding of cognitive accuracy in networks can contribute to the study of local emergency management networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(retrieval_method='pinecone', index_name=\"my-index-5\", embedding_model_name=\"all-MiniLM-L6-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_FP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
